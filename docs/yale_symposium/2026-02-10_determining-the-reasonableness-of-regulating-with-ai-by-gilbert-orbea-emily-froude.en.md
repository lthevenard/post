# Determining the Reasonableness of Regulating with AI

Autor(es): Gilbert Orbea; Emily Froude  
Data: February 10, 2026  
Fonte: https://www.yalejreg.com/nc/determining-the-reasonableness-of-regulating-with-ai-by-gilbert-orbea-emily-froude/

---

This post is the fifth contribution to Notice & Comment’s symposium on AI and the APA. For other posts in the series, click here.

In this symposium, we explore how the increased adoption of AI by federal agencies will implicate the Administrative Procedure Act. We attempt to address this question by focusing on how courts may respond to agency use of AI.

When reviewing agency action, courts generally consider whether an action was “reasonable” under the applicable standard of review. But how might courts determine “reasonableness” in the context of agency decision-making and AI? We find guidance in how courts currently review agency use of scientific and economic models, along with their underlying data and the procedures through which the models are developed.

As with any emerging technology, agency adoption of AI raises a host of new legal questions. These questions arise because AI differs in important ways from traditional predictive modeling, which involves the application of statistical analysis and methods to known and reported data. AI systems, by contrast, are typically built through iterative “training” on a dataset. By identifying patterns and relationships in a training dataset, AI systems can develop complex and nonlinear models capable of generating predictions, classifications, or decisions.

These systems may also evolve over time by continuously incorporating new data, potentially leading to changes in system performance and decision-making processes.

Against this background, we propose that when reviewing agency action that relies on AI, courts will look to three primary factors:

1. The nature of the statutory authority: Courts are more likely to express concern about agency use of AI when it involves significant discretion on value-laden matters rather than simple factfinding.
2. How the AI is used and what role it plays: The more significant the AI’s role, the more likely it is that courts will expect agencies to justify and explain it.
3. The subject matter and the consequences: When AI is used in high-stakes contexts implicating health, safety, or other substantial interests, courts may scrutinize its use more carefully.

No one factor is dispositive. In some cases, AI use might be acceptable because the agency uses it only as a screening tool or to identify issues for further review. In other cases, the use of AI could render the agency’s decision unreasonable if it plays a decisive role and the agency cannot meaningfully explain or validate its outputs.

We emphasize that the appropriate legal standards are not yet settled. Courts will likely develop new approaches as AI becomes more prevalent in agency decision-making, particularly if litigants begin to challenge agency actions on the ground that the agency relied on AI without adequate explanation or safeguards.

In sum, our proposal aims to provide a starting point for thinking about how courts will assess agency use of AI under the APA. The more discretionary the underlying authority, the more decisive the role of AI, and the more significant the consequences, the more courts may demand transparency and justification.

Gilbert Orbea is a Litigation Associate at WilmerHale and an adjunct professor at Georgetown Law School; Emily Froude is an Associate Professor of Law at The Ohio State University Moritz College of Law.
