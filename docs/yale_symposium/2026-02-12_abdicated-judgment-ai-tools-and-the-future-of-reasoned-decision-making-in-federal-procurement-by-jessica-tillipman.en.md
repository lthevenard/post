# Abdicated Judgment: AI Tools and the Future of Reasoned Decision-Making in Federal Procurement

Autor(es): Jessica Tillipman  
Data: February 12, 2026  
Fonte: https://www.yalejreg.com/nc/abdicated-judgment-ai-tools-and-the-future-of-reasoned-decision-making-in-federal-procurement-by-jessica-tillipman/

---

This post is the sixth contribution to Notice & Comment’s symposium on AI and the APA. For other posts in the series, click here.

Federal agencies are rapidly expanding their use of artificial intelligence to support procurement decisions. From automated proposal scoring to risk assessments and responsibility determinations, AI tools promise speed, consistency, and efficiency. But they also raise a fundamental administrative law concern: reasoned decisionmaking.

Procurement decisions—especially best-value tradeoffs—often require context-specific judgment. Agencies must explain why they made certain choices, how they weighed factors, and why the selected proposal represents the best value. If AI tools drive these judgments, agencies may find themselves unable to defend their decisions when challenged.

Consider a hypothetical: an agency uses an AI tool to evaluate proposals and recommend an award. The source selection authority (SSA) signs the decision. A disappointed offeror files a protest. The record contains extensive AI-generated documentation: scores, narratives, and summaries. But when asked to explain why the agency preferred one proposal over another, the SSA cannot articulate the underlying rationale beyond the AI’s outputs.

This creates an “award decision no one can defend.” The problem is not just opacity; it is that the decisionmaker may have abdicated judgment to the tool.

AI may also create a “documentation paradox.” AI can generate more text than humans ever could, making it appear that the agency engaged deeply with the record. But more words do not necessarily mean more reasoning. AI-generated narratives can be generic, conclusory, or disconnected from the actual evaluation criteria.

This is particularly acute because procurement law already requires reasoned decisionmaking. FAR 15.308 provides:

“The source selection decision shall be documented, and the documentation shall include the rationale for any business judgments and tradeoffs made or relied on by the SSA, including benefits associated with additional costs.”

If the SSA is only rubber-stamping AI outputs, the documentation may not reflect real judgment. A protest tribunal may find the decision unreasonable or inadequately explained.

So what is to be done? Agencies need a framework for validating AI use in procurement. That framework should include:

• Defining the AI tool’s role: supportive, not substitutive.
• Ensuring transparency and recordability: the agency must be able to explain how the tool works and how it was used.
• Testing and validation: including bias evaluation and performance metrics.
• Preserving independent decisionmaking: the SSA must engage with the evidence and make the tradeoffs.

AI can be valuable for routine tasks, data synthesis, and consistency checks. But if it becomes the de facto decisionmaker, reasoned decisionmaking is at risk.

Jessica Tillipman is Associate Dean for Government Procurement Law Studies and an Associate Professor of Law at the George Washington University Law School.
