# Symposium on AI and the APA — argumentos centrais (ultra-resumo)

Baseado nos resumos em `docs/yale_symposium/*.summary.pt.md`.

## Diálogo em ordem cronológica

- **2026-02-05 — Bridget C.E. Dooling & Jordan Ascher (Introdução):** a IA já está entrando no cotidiano das agências, e o simpósio pergunta como o APA deve lidar com ganhos de eficiência sem perder accountability e qualidade decisória.
- **2026-02-06 — Cary Coglianese:** “LLMs são como taxistas opinativos”: sem evidência, validação e record, outputs convincentes não satisfazem o APA—IA serve como apoio, não como fundamento automático.
- **2026-02-06 — Jordan Ascher & John Lewis:** aprofundam o alerta: o APA pressupõe deliberação humana e exige justificar/validar modelos e prompts; “carimbar” respostas de LLM (inclusive a comentários) torna a regra vulnerável.
- **2026-02-09 — Jack Jones & Burçin Ünel:** contraponto pragmático: o APA não veda IA *per se* e cortes tendem a olhar para o produto final; o risco é a opacidade/sycophancy—daí a necessidade de supervisão e transparência.
- **2026-02-10 — Gilbert Orbea & Emily Froude:** propõem como o Judiciário pode calibrar “razoabilidade” do uso de IA: autoridade (discrição), papel da IA (quão decisivo) e consequências (quão altas) determinam o nível de exigência de explicação e salvaguardas.
- **2026-02-11 — Elliot E.C. Ping:** ceticismo sobre a “solução” técnica: chain-of-thought pode fabricar transparência e manter hallucinations; explicações mais longas não garantem razão verificável nem compliance com o APA/Chenery.
- **2026-02-11 — Adam Crews:** em vez de cortes inventarem novos procedimentos (Vermont Yankee), padrões de governança de IA devem emergir de práticas das próprias agências (com apoio do ACUS), que depois podem ser reconhecidas judicialmente.
- **2026-02-12 — Reeve T. Bull:** desloca o foco para usos produtivos: IA pode apoiar reforma regulatória “cirúrgica” (classificar obrigações, heat maps, revisão retrospectiva), mas não viabiliza promessas maximalistas de cortes em massa.
- **2026-02-12 — Jessica Tillipman:** aplica o problema ao procurement: se a racionalidade fica “dentro” de um modelo opaco, a agência pode não conseguir defender tradeoffs; é preciso auditabilidade, validação e julgamento humano real (FAR 15.308).
- **2026-02-13 — Tara Aida:** alerta específico do §553: automatizar notice-and-comment pode “ticar caixas” sem engajamento—e prompts podem institucionalizar closed-mindedness, esvaziando a função substantiva do procedimento.
- **2026-02-16 — Joe Tomlinson & Brendan McGurk:** perspectiva comparada: o UK tenta uma abordagem pro-innovation + registros de transparência, mas o patchwork não resolve opacidade e lacunas doutrinárias; pode haver pressão por um framework mais dedicado.
- **2026-02-16 — Bridget C.E. Dooling:** fecha o “diálogo” criticando a retórica: “use cases” e “human in the loop” não são justificativa; IA não decide valores—boa governança exige saber dizer “não” e manter humanos no centro da responsabilidade.

