# Self-driving cars, Miami drivers and uses of AI in academic research

![Illustration: Self-driving car](./assets/posts/2026-self-driving-cars/cover.png)

<br>

There are two versions of the “AI in academia” debate.

There is the public one, full of solemn warnings about black boxes, bias, hallucinations, plagiarism, privacy, and the end of human judgment. And there is the private one—usually held after a seminar, near the coffee—where everyone quietly admits they’re already using AI tools in their own research, and then quickly adds (with the sincerity of a sworn affidavit) that it’s only for “brainstorming,” “summarizing,” and “grammar.”

Still, let’s start with the public version, because the concerns are real.

## 1) The black box problem (transparency)

Generative AI systems are often opaque: training data is hard to audit, model internals are not interpretable in any simple way, and even “explanations” can be post-hoc stories invented by the model to justify itself. In research, this raises a basic methodological anxiety: *if we can’t see how the answer was produced, what exactly are we evaluating?*

Fortunately, academic research—especially in the social sciences—has already solved transparency. Methods sections are famously detailed and fully replicable. Data is always cleanly documented and shared. Code is consistently archived. Analytic choices are never made under time pressure, and certainly never revised after the results look “weird.” Reviewers, for their part, are known for reading every appendix with loving attention and for immediately rewarding any author who says: “here are the messy details.”

In other words: compared to the pristine glass house of contemporary scholarship, the AI “black box” looks downright unprofessional.

## 2) Bias (the search for the neutral analyst)

AI can reproduce biases embedded in its training data and in the prompts users give it. If you ask a system to summarize “the literature,” it may summarize *a* literature: the one most represented online, most cited, most available in English, and most compatible with generic patterns of reasoning. Bias can enter quietly, as a tone of certainty, a default framing, or a set of assumptions smuggled in as “common sense.”

By contrast, social scientists are famously immune to these problems. Human researchers never fall in love with their hypotheses. They do not privilege convenient datasets. They do not mistake what is measurable for what matters. They do not adopt inherited categories because “that’s what the field usually uses.” They certainly do not treat canonical authors as if they were inevitable laws of nature. And they *definitely* do not interpret ambiguous evidence in ways that just happen to align with their priors, their politics, or their careers.

Bias, in short, is a machine-generated problem. Humans merely have deeply thought, carefully examined “theoretical commitments.”

## 3) Hallucinations (and the citation that never existed)

Large language models can generate false statements in a confident, well-written tone. Worse, they can produce plausible-sounding citations that do not exist—or that exist but do not say what the model implies. For academic work, this is a nightmare: scholarship is made of references, and references are made of trust.

To be clear, this is not a purely hypothetical fear. Studies have found that ChatGPT can fabricate a non-trivial share of citations when asked for sources, with error rates varying by model and prompt. (Which is why “please provide citations” should be treated as a request for *leads to verify*, not as a bibliography.)

This is particularly alarming when we compare AI uses to human analysis. As we all know, human scholars never cite papers they haven’t fully read. Nobody has ever cited a famous work based solely on its title, or on other citations. Nobody has ever copy-pasted a reference from another PDF without checking the page number. And nobody, in the entire history of graduate education, has ever written: “as X convincingly demonstrates,” when what X actually demonstrates is that the author has not read what is being cited.

Again, those damn machines introducing horrible mistakes that humans would never produce on their own.

## 4) The plausible paragraph problem (originality, clichés, and consensus)

AI is very good at producing paragraphs that sound like scholarship: balanced, cautious, properly hedged, and filled with phrases such as “the literature suggests.” The danger is not just factual error—it is intellectual flattening. A model trained to predict plausible text may reproduce conventional wisdom, overstate consensus, understate uncertainty, and smooth over the weirdness that often matters most in social inquiry.

Thankfully, human researchers have never been tempted by the plausible paragraph. Literature reviews are known for their radical originality, their fearless engagement with dissenting views, and their refusal to recycle field-specific clichés. No one has ever written “in an increasingly complex world” in the opening sentence of a social science paper. No one has ever assembled an argument by stacking five respectable-sounding citations like decorative pillows. And no one has ever confused “this is widely said” with “this is true.”

If anything, our greatest risk is *too much* intellectual risk-taking by the pioneering minds of contemporary scholars.

## 5) Accountability (who is responsible for the output?)

When an AI system generates text, who owns it? Who is responsible for errors? What does it even mean to “stand behind” a paragraph you did not write? These are serious questions—especially when AI is used not merely to polish prose, but to propose claims, synthesize evidence, or draft sections that look like analysis.

Here again, academia offers a reassuring benchmark. Responsibility in research is always crystal clear. Authorship is perfectly aligned with contribution. Incentives are beautifully structured to reward slow, careful thinking. And when errors occur, correction mechanisms work swiftly and harmoniously, without defensiveness, social dynamics, or institutional anxiety.

In short: if you worry that AI obscures accountability, you should take comfort in the fact that we humans have already solved it a long time ago.

## 6) Ethics and confidentiality (data goes somewhere)

Many AI tools are third-party services. Feeding them sensitive data—interview transcripts, administrative records, student work, unpublished manuscripts—raises privacy and confidentiality concerns. It can also create compliance problems when research involves human subjects, protected data, or contractual restrictions.

But let’s be honest: researchers have a long, proud history of impeccable data hygiene. Nobody stores identifiable data in a folder called 'final_really_final_interviews/'. Nobody uploads confidential documents “just to summarize them.” Nobody shares datasets through informal channels when the formal process feels slow. And no one has ever discovered, five years later, that the only copy of a crucial dataset lived on someone’s laptop, which was last seen at an airport security checkpoint.

So yes—AI introduces new ways to mishandle data. The important thing is to preserve our old ones.

## My actual point: AI versus Miami drivers

<div class="image-text-block image-text-block--text-first">
  <img
    src="./assets/posts/2026-self-driving-cars/driver.png"
    alt="Illustration: A AI version of the 'Miami Driver'"
    class="image-text-block__image"
    style="border: 1px solid white;"
  />

  <div class="image-text-block__text">
    <p>
        This week I read Dave Barry’s Substack post, <a href="https://davebarry.substack.com/p/my-waymo-adventure">“My Waymo Adventure”</a> (Feb. 20, 2026), about self-driving cars. After describing the general terror of human driving—and, in particular, Miami driving—he makes the pro-autonomy case with a single, devastating punchline:
    </p>
    <p style="padding-left: 10px; border-left: 2px solid #22d3ee;">
    <cite>
        “Even if they’re not perfect — even if there’s some “glitch” in their software that causes them, at random times, to deliberately aim at pedestrians or oncoming trucks — they would still be above-average drivers in Miami.”
    </cite>
    </p>
    <p>
        I keep thinking about that line because it neatly reframes how one should think about employing AI tools in academic research.  
    </p>
    <p>
        The standard framing compares AI to an idealized version of scholarship: perfectly transparent, unbiased, meticulously cited, methodologically pristine, and ethically flawless. In that comparison, AI inspires fear and mistrust, because *of course it does*. The imagined benchmark is not “good research.” The imagined benchmark is “research conducted by angels with tenure.”
    </p>
  </div>
</div>

But the relevant comparison is not AI versus angels. It is AI versus *Miami drivers*—that is, AI versus the baseline reality of how research is actually produced in the social sciences: under constraints, incentives, fatigue, hierarchy, and human cognitive limitations.

That baseline includes well-known failure modes. Some are structural (publish-or-perish incentives, publication bias, the professional rewards of novelty). Some are cognitive (confirmation bias, motivated reasoning, the lure of tidy narratives). Some are methodological (flexible analytic choices, selective reporting, underpowered designs). Entire reform movements—open science, preregistration, registered reports, replication initiatives—exist because the field has had to admit that these are not rare anomalies. They are predictable consequences of humans doing difficult inference in institutional settings.

Once you start from that baseline, the conversation about AI becomes more honest—and more useful.

One thing that helps is to stop treating “AI in research” as a single practice. There is a meaningful difference between using AI as an **analysis tool** and using AI as a **text/argument machine**.

**AI as analysis.** When you use AI to read texts, code interviews, classify documents, extract variables, or speed up the implementation of explanatory models and plots, you are effectively inserting it into your measurement and computation pipeline. The central question is methodological: validity, reliability, and replicability. If a model helps produce your variables, then the model is part of your operationalization—and it should be treated like any other measurement instrument: calibrated, audited, and stress-tested.

**AI as writing.** When you use AI to draft the argument itself—or to produce the final prose of an academic publication—the core risk shifts. It is not only that the text might contain mistakes. It is that scholarship drifts into “machines talking to machines”: authors generating papers, reviewers generating reviews, editors generating decision letters, and nobody doing the slow work of thinking (and taking responsibility) that academic writing is supposed to represent.

It is still true that AI can hallucinate. So can humans, except we call it “misremembering the literature” or “overgeneralizing from a case.” It is still true that AI can be biased. So can humans, except we call it “a theoretical perspective” or “a research agenda.” It is still true that AI can be a black box. So can humans, except the box is called “tacit knowledge,” and the training data is called “my entire intellectual formation since age 19.”

The point is not that “AI is fine because humans are bad.” The point is that **rigor is not a purity test**. It is a set of practices that reduce error *relative to a real baseline*.

## In conclusion, do whatever you want! (just kidding)

When AI is used for analysis, rigor looks like measurement discipline: validation, error estimates, and robustness checks. When AI is used for writing, rigor looks like accountability: a human author who can defend what the text claims.

So yes: we should demand transparency, verification, ethical safeguards, and disclosure when AI is used in research. But we should also stop being so terrified of conditions that (1) we do not currently meet, and (2) we often do not even reliably incentivize.

It is understandable to fear the effects of AI, because it can lower standards. But it can also raise them dramatically, especially for tasks where the current baseline is already a mix of haste, opacity, and informal shortcuts.

And if you find yourself longing for the comfort of an error-free research world, remember: even if the robots aren’t perfect, they might still be well above average drivers in Miami.

## Further reading (for when the sarcasm wears off, or to revive it)

- Dave Barry, [“My Waymo Adventure”](https://davebarry.substack.com/p/my-waymo-adventure) (Substack, Feb. 20, 2026)
- Open Science Collaboration, [“Estimating the reproducibility of psychological science”](https://pubmed.ncbi.nlm.nih.gov/26315443/) (*Science*, 2015)
- Simmons, Nelson & Simonsohn, [“False-Positive Psychology”](https://pubmed.ncbi.nlm.nih.gov/22006061/) (*Psychological Science*, 2011)
- Gao et al., [“ChatGPT-3.5 and ChatGPT-4: Medical Science and Potential for Fabricated Citations”](https://pubmed.ncbi.nlm.nih.gov/37666767/) (Cureus, 2023)
- NIST, [Artificial Intelligence Risk Management Framework (AI RMF 1.0)](https://www.nist.gov/publications/artificial-intelligence-risk-management-framework-ai-rmf-10) (2023)
