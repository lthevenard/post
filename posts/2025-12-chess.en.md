# Chess, Uncertainty, and Pub Philosophy

![Illustration: chess in Plato's cave](./assets/posts/2025-chess/platos-cave-chess.png)

### Too much chess on the holidays

At the end of 2025, sometime between holiday leftovers and the vague promise that next year would be calmer, I made a questionable life choice: I started playing a lot of chess. Not the dignified, two-games-a-week kind of chess, but the slightly obsessive version—rapid games on my laptop, long late-night sessions, and the uncomfortable realization that “just one more game” is a lie we tell ourselves far more often than we should.

At first, chess felt reassuringly simple. The pieces behave, the board is orderly, and nothing happens unless someone makes a move. Coming from a professional life spent thinking about regulation, decision-making, and institutional complexity, this was comforting. Surely a game with fixed rules, perfect information, and no bureaucratic procedures would be a relaxing pastime.

This illusion, however, was short-lived.

As I started studying the game more seriously—watching lectures, analyzing losses, and discovering entire categories of mistakes I didn’t know I was making—it became clear that chess is less about knowing the rules and more about navigating uncertainty under severe cognitive constraints. You can see everything, and still understand almost nothing. You can make a move that is locally sensible and strategically disastrous. You can prepare carefully and lose to something you’ve “seen before”, but failed to recognize in time.

Somewhere between hanging my queen for the fifth time and confidently executing a plan that was objectively terrible, I realized that chess is not just a game. It is a compact, brutally honest laboratory of decision-making. One that punishes overconfidence, rewards disciplined thinking, and exposes the limits of both intuition and calculation—often within the span of a single game.

This post begins with chess, but it is not really about chess. It is about what studying the game made me think about: bounded rationality, learning through failure, the difference between formal rules and effective strategies, and why having more information does not necessarily make decisions easier. The chess board is just another place in which these ideas come out to play.

### Wait, this feels familiar...

One of the reasons chess is so tempting as a metaphor for understanding reality is that it looks profoundly objective. The rules are fixed, the pieces are known, the board is finite, and there is no hidden information. Nothing is happening behind your back. In principle, everything that matters is right there in front of you.

And yet, the moment you try to understand a position, objectivity quietly slips away.

Faced with a chessboard, you are forced to make choices long before you make a move. What matters here? King safety or material balance? Space or initiative? A long-term structural weakness or a short-term tactical threat? Two players can look at the exact same position—with identical information—and see entirely different games unfolding. The disagreement is not about the rules; it is about relevance. Chess, it turns out, is less a game of perfect information than a game of selective attention.

<div style="display: flex; justify-content: center;">
  <img
    src="./assets/posts/2025-chess/chess-brains.png"
    alt="Illustration: the overthinking maniac."
    style="width: 70%; max-width: 600px;"
    loading="lazy"
  />
</div>

This is where things start to feel epistemologically uncomfortable. Although the game is governed by strict, formal rules, any actual understanding of a position depends on heuristics, abstractions, and simplifications. We talk about “good bishops,” “weak squares,” and “dangerous attacks” not because these concepts are written into the rules, but because the raw complexity of the game forces us to compress reality into something cognitively manageable. What feels like a neutral evaluation is often the product of deeply subjective framing decisions.

The scale of that complexity is not just rhetorical. By around move 20, the number of possible continuations of a chess game becomes astronomically large—so large that the total number of distinct possible games of chess (often referred to as the Shannon number) is commonly estimated to be greater than 10¹²⁰. That figure comfortably exceeds widely cited estimates of the number of atoms in the observable universe. In other words, chess is a small, finite system that very quickly escapes any hope of exhaustive understanding.

This creates a strange tension. In theory, chess is fully solvable: with unlimited computational power and time, one could evaluate every possible continuation and determine the objectively best move in every position. In practice, this theoretical solution is about as useful to a human player as knowing that, somewhere, there exists a complete map of reality written in a language we cannot read.

So we do what humans always do. We build models. We rely on patterns. We trust intuition, then second-guess it. We mistake familiarity for understanding and confidence for correctness. Sitting in front of the board, armed with concepts and principles we know are imperfect, we make the best decisions we can. But we are always aware, at some level, that we are reasoning inside a cave whose walls we mistake for the world itself. And, unlike Plato's philosopher, we can never hope to turn our heads to see the light. It is just too bright for our eyes.

The game of chess does not just illustrate the limits of calculation. It illustrates something more unsettling: that even in a universe with fixed rules and no hidden information, our access to “objective reality” is mediated by the choices we make about what to see, what to ignore, and what we are capable of understanding at all.


### Reasoning about what you cannot understand

Once you accept that chess cannot really be understood in any complete sense, an awkward question follows: how, then, are we supposed to study it?

The traditional answer is concepts. Chess literature is full of them, generously supplied by brilliant players with an impressive talent for naming things. We are told to gain space, to keep the initiative, to improve our worst-placed piece, to restrict counterplay through prophylaxis. These ideas are presented not as abstract theory, but as living principles, illustrated through beautifully annotated games in which past masters demonstrate how “correct understanding” quietly suffocates their opponents.

There is something deeply reassuring about this. Concepts give structure to chaos. They turn an incomprehensible search space into a narrative we can follow: White had more space, Black lacked counterplay, therefore the end was inevitable. The board becomes legible again, almost moral. Good plans are rewarded; bad ones are punished. The world is fair, after all... right?

Then the engines arrived and ruined the party.

Modern chess engines, armed with inhuman patience and zero respect for conceptual elegance, have shown that many of these classical analyses are simply wrong. Entire plans collapse after a single “ugly” move no human would naturally consider. Positions praised for decades turn out to be objectively equal—or worse. The computer does not care about initiative, harmony, or long-term compensation. It just calculates. Relentlessly. Concretely.

At first glance, this seems devastating. If the analyses are mistaken, why study them at all? Why learn concepts that fail under perfect calculation?

The answer, inconveniently, depends on what we think knowledge is good for.

If our goal were to grasp the objective truth of a chess position—its final, correct evaluation—then yes, many classical books would belong in a small museum of historically interesting mistakes. But that is not our goal. Our goal is much more modest and much more human: we want to make better decisions than the person sitting across the board from us.

From that perspective, much of what the engine “knows” is useless. A refutation that requires calculating a twenty-move sequence with multiple only moves might be objectively decisive, but it has no practical value if no human can reliably find it. Concepts survive not because they are perfectly true, but because they are usable. They compress reality in ways that align with our cognitive limits, allowing us to act effectively under uncertainty.

In this sense, studying chess concepts is not about learning the truth of the position. It is about learning how to think in positions like it.

This is where the metaphor escapes the chessboard. In real life, our reasoning is also built on oversimplifications. We rely on categories, narratives, heuristics, and mental shortcuts that we know—if we are honest—are incomplete, biased, and occasionally wrong. We cannot calculate everything. We cannot see every consequence. Like chess players, we operate with concepts that gesture toward reality without ever fully capturing it.

But imperfection does not make reasoning worthless. It makes it necessary. The real mistake is not using crude models of the world; it is forgetting that they are crude. Wisdom, whether in chess or in life, seems to lie somewhere between two extremes: the naïve belief that our concepts perfectly describe reality, and the paralyzing conclusion that, because they do not, we should abandon them altogether.

We play the game of life with the tools we have. And sometimes, imperfect understanding is not a flaw—it is the only way understanding can exist at all.

### AI? Really? Is everything about AI these days?

Elon Musk once dismissed chess, remarking in an interview that a cell phone can now beat any grandmaster (which is true and can be deeply unsettling for players who devoted their whole lives to the game). In chess—as in many other domains—AI outperforms humans not merely because it is faster or more patient, but because it can dispense entirely with concepts and rely instead on brute-force evaluation of vast numbers of relevant variables.

This is a remarkable achievement, and in many contexts it allows humans to act more effectively than they ever could on their own. Yet it also carries a quiet risk: that we begin to distrust, or even abandon, our own capacity to interpret the world, mistaking conceptual understanding for an inferior substitute for calculation. If knowledge becomes something we only consult rather than something we practice, we risk turning human judgment into a thin interface between reality and a machine that “knows better.” Let us hope, then, that AI does not suffocate human knowledge by rendering it obsolete, but instead complements it—extending our reach without erasing the imperfect, conceptual, and deeply human ways in which we make sense of the world.

<div class="image-text-block">
  <img
    src="./assets/posts/2025-chess/meme_en.jpeg"
    alt="Illustration: Yay, I won a game!"
    class="image-text-block__image"
    style="border: 2px solid white;"
  />

  <div class="image-text-block__text">
    <p>
      At this point, it is probably fair to admit that I may have taken chess a bit too far. What started as a harmless holiday distraction somehow escalated into epistemology, Plato’s cave, and a brief flirtation with the future of humanity in the age of artificial intelligence. This is, admittedly, not the most normal trajectory for a board game involving wooden horses and a queen that moves in straight lines. One could reasonably suspect that this entire post is less the result of deep insight and more the predictable consequence of too much spare time, too few social commitments, and my own propensity to overthink everything that I do.
    </p>
    <p>
      Still, if chess taught me anything, it is that overthinking is not a bug—it is a feature, provided we do not confuse it with wisdom. So now, having exhausted both the metaphor and my own self-restraint, I will do the responsible thing and end this post.
    </p>
    <p>
      Go play some chess. Have fun. Just, please, don’t look at my rating—I know far more about regulatory procedures than I do about moving chess pieces.
    </p>
  </div>
</div>


