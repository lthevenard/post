# Carros autônomos, motoristas de Miami e usos de IA em pesquisa acadêmica

![Ilustração: carro autônomo](./assets/posts/2026-self-driving-cars/cover.png)

<br>

Existem duas versões do debate sobre “IA na academia”.

Há a versão pública, cheia de advertências solenes sobre caixas-pretas, vieses, alucinações, plágio, privacidade e o fim do julgamento humano. E há a versão privada—geralmente depois de um seminário, perto do café—em que todo mundo admite, baixinho, que já está usando ferramentas de IA na própria pesquisa e, em seguida, acrescenta (com a sinceridade de uma declaração juramentada) que é só para “brainstorming”, “sumarização” e “gramática”.

Ainda assim, vamos começar pela versão pública, porque as preocupações são reais.

## 1) O problema da caixa-preta (transparência)

Sistemas de IA generativa costumam ser opacos: os dados de treinamento são difíceis de auditar, o funcionamento interno dos modelos não é interpretável de forma simples e até mesmo “explicações” podem ser histórias pós-hoc inventadas pelo próprio modelo para se justificar. Em pesquisa, isso gera uma ansiedade metodológica básica: *se não conseguimos ver como a resposta foi produzida, o que exatamente estamos avaliando?*

Felizmente, a pesquisa acadêmica—especialmente nas ciências sociais—já resolveu o problema da transparência. Seções de metodologia são notoriamente detalhadas e plenamente replicáveis. Dados são sempre limpos, bem documentados e compartilhados. Código é consistentemente arquivado. Escolhas analíticas nunca são feitas sob pressão de tempo e, certamente, nunca são revisadas depois que os resultados parecem “estranhos”. Pareceristas, por sua vez, são conhecidos por ler cada apêndice com carinho e por recompensar imediatamente qualquer autor que diga: “aqui estão os problemas da minha pesquisa”.

Em outras palavras: comparada à casa de vidro impecável da pesquisa contemporânea, a “caixa-preta” da IA parece francamente amadora.

## 2) Viés (a busca pelo analista neutro)

A IA pode reproduzir vieses embutidos em seus dados de treinamento e nos prompts que os usuários fornecem. Se você pede a um sistema para resumir “a literatura”, ele pode resumir *uma* literatura: a mais frequentemente representada online, a mais citada, a mais disponível em inglês e a mais compatível com padrões genéricos de raciocínio. O viés pode entrar silenciosamente na análise, como um tom de certeza, um enquadramento padrão ou um conjunto de premissas contrabandeadas do “senso comum”.

Em contraste, cientistas sociais são notoriamente imunes a esse tipo de problema. Pesquisadores humanos nunca se apaixonam por suas hipóteses. Não privilegiam bases de dados convenientes. Não confundem o que é mensurável com o que importa. Não adotam categorias herdadas porque “é o que a área normalmente usa”. Certamente não tratam autores canônicos como se fossem leis inevitáveis da natureza. E *definitivamente* não interpretam evidências ambíguas de maneiras que, por pura coincidência, se alinham com seus *priors*, suas visões políticas ou interesses de carreira.

Viés, em suma, é um problema que foi gerado pelas máquinas. Humanos apenas detêm alguns “compromissos teóricos”, mas eles sempre são fruto de uma reflexão profunda e cuidadosa.

## 3) Alucinações (e a citação que nunca existiu)

Grandes modelos de linguagem podem gerar afirmações falsas com um tom confiante e um texto bem escrito. Pior: eles podem produzir citações plausíveis que não existem—ou que existem, mas não dizem o que o modelo sugere. Para o trabalho acadêmico, isso é um pesadelo: a produção acadêmica é feita de referências, e referências são feitas de confiança.

Para deixar claro: isso não é um medo puramente hipotético. Estudos já mostraram que o ChatGPT pode fabricar uma parcela não trivial de citações quando é solicitado a fornecer fontes, com taxas de erro que variam por modelo e prompt (e é por isso que “por favor, forneça citações” deveria ser tratado como um pedido de *pistas para verificar*, não como uma bibliografia).

Esse problema é particularmente alarmante quando comparamos o uso de IA com a análise humana. Como todos sabem, pesquisadores humanos nunca citam artigos que não leram por completo. Ninguém jamais citou uma obra famosa com base apenas em seu título, ou em outras citações. Ninguém jamais copiou e colou uma referência de outro PDF sem checar o número da página. E ninguém, em toda a história dos cursos de pós-graduação, jamais escreveu: “como X demonstra convincentemente,” quando o que X demonstra é que o autor não leu o que está citando.

De novo: essas malditas máquinas introduzindo erros horríveis que humanos nunca produziriam por conta própria...

## 4) O problema do parágrafo plausível (originalidade, clichês e consenso)

A IA é muito boa em produzir parágrafos que soam como pesquisa: equilibrados, cautelosos, devidamente cheios de “ressalvas” e repletos de expressões como “a literatura sugere”. O risco não é apenas o erro factual—é o achatamento intelectual. Um modelo treinado para prever texto plausível tende a reproduzir senso comum, exagerar consenso, subestimar incerteza e suavizar a estranheza que, muitas vezes, é precisamente o que mais importa na investigação social.

Felizmente, pesquisadores humanos nunca foram tentados pelo parágrafo plausível. Revisões de literatura são conhecidas por sua radical originalidade, seu engajamento destemido com visões dissidentes e sua recusa em reciclar clichês da área. Ninguém jamais escreveu “em um mundo cada vez mais complexo” na primeira frase de um artigo. Ninguém jamais montou um argumento empilhando cinco citações respeitáveis como almofadas decorativas. E ninguém jamais confundiu “isso é muito dito” com “isso é verdade”.

Se existe algum risco atualmente, é o da ousadia intelectual *excessiva* das mentes pioneiras da academia contemporânea.

## 5) Responsabilização (quem é responsável pelo resultado?)

Quando um sistema de IA gera texto, quem é o dono? Quem responde por erros? O que significa “assinar embaixo” de um parágrafo que você não escreveu? Essas são questões sérias—especialmente quando a IA não é usada apenas para polir a escrita, mas para propor afirmações, sintetizar evidências ou redigir seções que parecem análise.

Aqui também a academia oferece um parâmetro reconfortante. A responsabilidade em pesquisa sempre foi cristalina. A autoria, na academia, está sempre perfeitamente alinhada com a contribuição de cada autor. Os incentivos são lindamente estruturados para recompensar pensamento lento e cuidadoso. E, quando erros acontecem, mecanismos de correção funcionam de forma rápida e harmoniosa, sem defensividade, dinâmicas sociais perversas ou ansiedade institucional.

Em suma: se você se preocupa que a IA obscureça o significado da autoria acadêmica e a responsabilização pelos seus trabalhos, console-se com o fato de que nós, humanos, já resolvemos isso há muito tempo.

## 6) Ética e confidencialidade (os dados vão para algum lugar)

Muitas ferramentas de IA são serviços de terceiros. Alimentá-las com dados sensíveis—transcrições de entrevistas, registros administrativos, trabalhos de alunos, manuscritos não publicados é algo que levanta preocupações de privacidade e confidencialidade. Essas práticas também podem criar problemas de conformidade quando a pesquisa envolve participantes humanos, dados protegidos ou restrições contratuais.

Mas sejamos honestos: pesquisadores têm uma longa e orgulhosa história de higiene de dados impecável. Ninguém guarda dados identificáveis em uma pasta chamada 'versão_final_realmente_final_das_entrevistas/'. Ninguém faz upload de documentos confidenciais “só para resumir”. Ninguém compartilha datasets por canais informais quando o processo formal parece lento. E ninguém jamais descobriu, cinco anos depois, que a única cópia de um dataset crucial vivia no notebook de alguém, visto pela última vez em uma fila de raio-X no aeroporto.

Então sim: a IA introduz novas maneiras de lidar mal com dados. É fundamental preservar as antigas, pois elas funcionavam muito bem.

## Meu verdadeiro argumento: IA versus motoristas de Miami

<div class="image-text-block image-text-block--text-first">
  <img
    src="./assets/posts/2026-self-driving-cars/driver.png"
    alt="Ilustração: uma versão de IA do “motorista de Miami”"
    class="image-text-block__image"
    style="border: 1px solid white;"
  />

  <div class="image-text-block__text">
    <p>
        Esta semana eu li o post do Dave Barry no Substack, <a href="https://davebarry.substack.com/p/my-waymo-adventure">“My Waymo Adventure”</a> (20 de fevereiro de 2026), sobre carros autônomos. Depois de descrever o terror geral de dirigir (e, em particular, de dirigir em Miami) ele faz o argumento pró carros autônomos com um único e devastador golpe de humor:
    </p>
    <p style="padding-left: 10px; border-left: 2px solid #22d3ee;">
    <cite>
        “Mesmo que eles não sejam perfeitos — mesmo que haja alguma ‘falha’ no software que faça com que, em momentos aleatórios, eles deliberadamente apontem na direção de pedestres ou de caminhões vindo em sentido contrário — ainda assim os carros autônomos seriam motoristas acima da média em Miami.” (tradução própria do <a href="https://davebarry.substack.com/p/my-waymo-adventure">original</a> em inglês)
    </cite>
    </p>
    <p>
        Eu continuo pensando nessa frase porque ela reformula, de forma elegante, como deveríamos pensar sobre o uso de ferramentas de IA em pesquisa acadêmica.  
    </p>
    <p>
        O enquadramento padrão compara a IA a uma versão idealizada do trabalho acadêmico: perfeitamente transparente, imparcial, meticulosamente referenciada, metodologicamente impecável e eticamente irrepreensível. Nessa comparação, a IA inspira medo e desconfiança, porque *é claro que isso teria que acontecer*. O parâmetro imaginado não é “boa pesquisa”. O parâmetro imaginado é “pesquisa conduzida por anjos catedráticos”.
    </p>
  </div>
</div>

Mas a comparação relevante não é IA versus anjos. É IA versus *motoristas de Miami*, isto é, IA versus a linha de base realista de como a pesquisa é, de fato, produzida nas ciências sociais: sob restrições, incentivos desalinhados, fadiga, hierarquia e as limitações cognitivas humanas que já conhecemos.

Essa linha de base inclui falhas bem conhecidas. Algumas são estruturais (incentivos de *publish-or-perish*, viés de publicação, as recompensas profissionais da novidade). Algumas são cognitivas (viés de confirmação, raciocínio motivado, a sedução de narrativas “limpas”). Algumas são metodológicas (flexibilidade em escolhas analíticas, relato seletivo dos resultados, desenhos subdimensionados). Movimentos inteiros de reforma acadêmica (ciência aberta, preregistration, registered reports, iniciativas de replicação) existem porque a área precisou admitir que esses não são acidentes raros. São consequências previsíveis de humanos tentando fazer inferências difíceis dentro de instituições.

Quando partimos dessa linha de base, a conversa sobre usos de IA fica mais honesta (e mais útil).

Uma coisa que pode ajudar esse debate é parar de tratar “IA em pesquisa” como uma prática única. Há uma diferença substantiva entre usar IA como **ferramenta de análise** e usar IA como **gerador de texto/argumento**.

**IA como análise.** Quando você usa IA para ler textos, codificar entrevistas, classificar documentos, extrair variáveis ou acelerar a implementação de modelos explicativos e gráficos, você está, na prática, inserindo a IA no seu fluxo de mensuração e computação. As perguntas importantes, nesse caso, são metodológicas: validade, confiabilidade e replicabilidade. Se um modelo de linguagem ajuda a produzir suas variáveis, então o modelo faz parte da sua operacionalização e deveria ser tratado como qualquer outro instrumento de medida: deve ser calibrado, auditado e submetido a testes de estresse.

**IA como escrita.** Quando você usa IA para redigir o argumento em si, ou para produzir a prosa final de uma publicação acadêmica, o risco central muda. Não é apenas o fato de que o texto pode conter erros. O risco real é ético: a produção acadêmica pode escorregar para uma situação em que temos “máquinas conversando com máquinas”: autores gerando artigos, pareceristas gerando pareceres, editores gerando cartas de decisão, sem que ninguém faça o trabalho lento de pensar (e assumir responsabilidade) que deveria estar integrado à produção acadêmica.

É verdade que a IA pode alucinar. Mas humanos também, só que chamamos isso de “lembrar mal da literatura” ou “generalizar demais a partir de um caso”. É verdade que a IA pode ser enviesada. Humanos também, só que chamamos isso de “perspectiva teórica” ou “agenda de pesquisa”. Também é verdade que a IA pode ser uma caixa-preta. Humanos também, só que a caixa se chama “conhecimento tácito”, e os dados de treinamento se chamam “toda a minha formação intelectual desde os 19 anos”.

O meu ponto não é que “usar IA é permitido porque humanos são ruins”. O argumento que quero fazer aqui é de que **rigor acadêmico não é um teste de pureza**. É um conjunto de práticas que reduz erro *em relação a uma linha de base real*.

## Em conclusão, faça o que der na telha! (brincadeira)

Quando a IA é usada para análise, o rigor se parece com disciplina metodológica que usamos quando queremos mensurar a realidade: validação, estimativas de erro e testes de robustez. Quando a IA é usada para escrita, o rigor se torna a garantia de padrões éticos de pesquisa e da responsabilidade dos autores: um autor humano deve ser capaz de defender o que o texto afirma e, para isso, deve estar apto a reproduzir o resultado obtido (ter conhecimento e autonomia sobre o que produziu).

Então sim: precisamos exigir transparência, verificação, salvaguardas éticas e divulgação quando a IA é usada em pesquisa. Mas também precisamos parar de ficar tão apavorados com condições que (1) já não cumprimos atualmente e (2) muitas vezes nem sequer sabemos incentivar com consistência.

É compreensível temer os efeitos da IA, porque ela pode baixar padrões. Mas ela também pode elevá-los dramaticamente, especialmente em tarefas em que a linha de base já é uma mistura de pressa, opacidade e atalhos informais.

E, se você se pegar desejando o conforto de um mundo acadêmico sem erros, lembre: mesmo que os robôs não sejam perfeitos, talvez eles ainda sejam motoristas muito acima da média em Miami...

## Leituras adicionais (para quando o sarcasmo passar, ou para aumentá-lo)

- Dave Barry, [“My Waymo Adventure”](https://davebarry.substack.com/p/my-waymo-adventure) (Substack, Feb. 20, 2026)
- Open Science Collaboration, [“Estimating the reproducibility of psychological science”](https://pubmed.ncbi.nlm.nih.gov/26315443/) (*Science*, 2015)
- Simmons, Nelson & Simonsohn, [“False-Positive Psychology”](https://pubmed.ncbi.nlm.nih.gov/22006061/) (*Psychological Science*, 2011)
- Gao et al., [“ChatGPT-3.5 and ChatGPT-4: Medical Science and Potential for Fabricated Citations”](https://pubmed.ncbi.nlm.nih.gov/37666767/) (Cureus, 2023)
- NIST, [Artificial Intelligence Risk Management Framework (AI RMF 1.0)](https://www.nist.gov/publications/artificial-intelligence-risk-management-framework-ai-rmf-10) (2023)

