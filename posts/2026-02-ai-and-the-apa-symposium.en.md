# The Yale Journal on Regulation's “Symposium on AI and the APA”: Organizing an Emerging Debate

![Illustration: The "AI hand"](./assets/posts/2026-yale-symposium/cover.png)

<br>

Artificial intelligence has become a political keyword in many jurisdictions, especially in the United States. In current debates about the use of AI in public administration, it is alternately framed—by proponents and critics alike—as a promise of state capacity or as a shorthand for automating public functions without accountability.

That tension is no longer abstract. It now sits squarely inside the day-to-day machinery of the administrative state, shaping how we see rulemaking, procurement, enforcement, benefits administration, and the countless internal steps that precede and shape public-facing decisions.

One reason this debate feels urgent is that government use of AI is no longer speculative: it is already happening across agencies, and it is often framed as a response to familiar institutional constraints—limited staff, large workloads, and intense pressure (both political and judicial) to justify decisions in writing.

Against this backdrop, the Yale Journal on Regulation’s [Notice & Comment](https://www.yalejreg.com/nc/) blog launched a symposium series titled “[Symposium on AI and the APA](https://www.yalejreg.com/topic/symposium-on-ai-and-the-apa/).” The symposium aims to create an open dialogue about what happens when AI is used to assist with administrative procedures and public functions governed by the Administrative Procedure Act (APA) in the United States—a space “for AI skeptics, AI believers, and everyone in between” to discuss the present and future of governance amid rapidly evolving AI technologies.

In this post, I explore some of the main themes and ideas that have emerged in the symposium so far. The goal is descriptive: to organize the debate by highlighting where the contributors seem to converge and where the main legal fault lines are becoming clearer.

## 1. A politically charged debate

In the opening post of the series, Bridget C.E. Dooling and Jordan Ascher point to a familiar pattern: once a tool exists that can generate plausible text at scale, the temptation is to treat it as a shortcut through bureaucracy—especially through processes that have long been criticized as slow or overly burdensome. Their introduction reads like a field report from the current, AI-forward moment in federal governance: the White House calling for “Accelerating Federal Use of AI”; the Department of Veterans Affairs using AI as a blunt instrument to “munch” contracts; defense officials pledging deeper AI integration; agencies giving employees access to chatbots; the EPA saying it will use AI to facilitate review of public comments; and more ambitious (and more controversial) proposals or reports, such as DOGE’s plan to use AI to rescind half of all federal regulations and reporting that the Department of Transportation may use Google Gemini to draft rules (see Dooling & Ascher’s symposium introduction [here](https://www.yalejreg.com/nc/introduction-to-the-symposium-on-artificial-intelligence-and-the-administrative-procedure-act-by-bridget-c-e-dooling-jordan-ascher/)).

These examples matter not only because they suggest adoption, but because they reflect a specific political demand: make the administrative state move faster. In the Trump era, that demand is often expressed as a mix of innovation rhetoric (“modernization”) and anti-bureaucratic impatience (“cut red tape”). AI becomes attractive because it seems to compress the distance between a political decision and publication in the Federal Register—that is, between a political choice and a narrative that can survive review.

At the same time, the story is not simply that “technology arrived.” The story is also about *what AI is being asked to do*. Some of the most visible proposals and press accounts have framed AI not merely as a way to speed up clerical work, but as a way to accelerate high-stakes policymaking—especially in contexts where speed is itself a political goal (deregulation, rescission, and rapid rollout of new initiatives). Tara Aida, for example, uses DOGE’s “AI Deregulation Decision Tool” proposal as a starting point and warns against treating AI as a procedural solvent that can dissolve the most labor-intensive parts of rulemaking without changing the substance of what the APA is for. (See [Aida](https://www.yalejreg.com/nc/ticking-the-boxes-ai-and-the-notice-and-comment-process-by-tara-aida/))

Aida also situates the appeal of AI in a broader intellectual climate: critiques of administrative “over-proceduralization” have become more mainstream, making the promise of automation feel—at least rhetorically—like an overdue fix for a process many already consider too slow and too formal. (See [Aida](https://www.yalejreg.com/nc/ticking-the-boxes-ai-and-the-notice-and-comment-process-by-tara-aida/))

That is why administrative lawyers are paying attention: the hard questions are not about whether agencies can use a spellchecker. They are about whether agencies can use AI to do the work that the APA is designed to force agencies to do—work that is as much about judgment as it is about paperwork.

## 2. Divergence, emerging consensus, and clearer fault lines

The Yale Journal on Regulation’s “Symposium on AI and the APA” is a useful snapshot of the current debate among U.S. administrative lawyers about the uses of AI in public administration. It serves as a good entry point precisely because it does not speak with one voice. The contributors include academics and practitioners, and they approach the problem from different angles: doctrinal, institutional, technical, and comparative.

Even so, reading the articles published so far as part of that series, one can see at least a few **points of emerging consensus** among its authors:

- **The baseline legal demands do not disappear.** The APA’s requirements of reasoned decision-making, attention to the record, and responsiveness to legally significant inputs still set the floor, even if agencies use new tools to meet them. (See [Coglianese](https://www.yalejreg.com/nc/ai-taxi-drivers-and-administrative-law-by-cary-coglianese/) and [Jones & Ünel](https://www.yalejreg.com/nc/do-large-language-models-dream-of-the-administrative-procedure-act-by-jack-jones-burcin-unel/))
- **Context matters.** “Agency use of AI” is not one thing. Using AI to triage correspondence, to summarize public comments, to score procurement bids, or to draft rule text implicates different risks and different legal hooks. (See [Orbea & Froude](https://www.yalejreg.com/nc/determining-the-reasonableness-of-regulating-with-ai-by-gilbert-orbea-emily-froude/) and [Tillipman](https://www.yalejreg.com/nc/abdicated-judgment-ai-tools-and-the-future-of-reasoned-decision-making-in-federal-procurement-by-jessica-tillipman/))
- **“Human in the loop” is not a complete answer.** Several pieces treat the phrase as an incomplete governance theory: it can signal genuine oversight, but it can also become a rhetorical shield for rubber-stamping. (See [Dooling](https://www.yalejreg.com/nc/use-cases-humans-in-the-loop-and-other-sleights-of-hand-by-bridget-c-e-dooling/) and [Ascher & Lewis](https://www.yalejreg.com/nc/toward-minimum-administrative-law-standards-for-agency-usage-of-ai-by-jordan-ascher-john-lewis/))

Beyond these emerging agreements, the symposium also clarifies the **fault lines** that are likely to shape the next round of legal and policy debates on this issue. Those fault lines are the focus of the next section.

## 3. The debates taking shape

Below are the main themes that the symposium helps bring into focus. Each theme is presented as a set of questions—and, where it helps, as a dialogue among contributors who are speaking past, with, or against each other.

### 3.1. “Reasoned decision-making” vs. “text that sounds reasoned”

Administrative law often treats written explanation as a proxy for thought: agencies must “examine the relevant data” and “articulate a satisfactory explanation,” and courts review whether the stated reasons connect the record to the choice.

The symposium repeatedly returns to a worry that generative AI can **sever that proxy**. If LLMs make plausible prose cheap, it becomes easier to produce documents that *look* like reasoned decision-making without reflecting it.

Coglianese’s “taxi driver” analogy captures the intuition. LLMs can deliver confident, plausible answers—including on technical policy questions—but the APA’s demands are not satisfied by plausibility alone. The discipline of administrative law is precisely to insist on evidence, alternatives, and a defensible chain from facts to policy choice. (See [Coglianese](https://www.yalejreg.com/nc/ai-taxi-drivers-and-administrative-law-by-cary-coglianese/))

Ascher and Lewis push this further by emphasizing that “ultimate responsibility” cannot be outsourced—especially where the tool is known to have failure modes (hallucinations, bias, sycophancy, and context limitations). In their framing, the APA’s reason-giving requirements put pressure not only on the *substantive output* but also on whether the agency can plausibly show that humans actually engaged with the relevant material rather than rubber-stamping a machine-generated narrative. (See [Ascher & Lewis](https://www.yalejreg.com/nc/toward-minimum-administrative-law-standards-for-agency-usage-of-ai-by-jordan-ascher-john-lewis/))

Tillipman makes the same structural point through a concrete setting—federal procurement—where decision-makers already have to document tradeoffs. AI can generate documentation, but documentation can become performative if it is not tied to human judgment that can be defended under scrutiny. (See [Tillipman](https://www.yalejreg.com/nc/abdicated-judgment-ai-tools-and-the-future-of-reasoned-decision-making-in-federal-procurement-by-jessica-tillipman/))

And Dooling adds a broader institutional critique: “human in the loop” can become a way to avoid asking what, exactly, humans owe the public in terms of values-laden judgment. If the human is merely a signatory, the loop is not doing the work that administrative law assumes it is doing. (See [Dooling](https://www.yalejreg.com/nc/use-cases-humans-in-the-loop-and-other-sleights-of-hand-by-bridget-c-e-dooling/))

### 3.2. The “black box” problem—and why technical fixes may not be legal fixes

One of the oldest concerns about AI in government is opacity: if neither the public nor the agency can explain how a system produced a result, how can the system be reviewed, contested, or trusted?

Elliot E.C. Ping’s contribution focuses on a contemporary attempt to tame that opacity: chain-of-thought prompting. In the AI community, chain-of-thought is often treated as a way to make models “show their work.” Ping’s caution, as read through administrative law, is that *more explanation is not necessarily better explanation*—because the “reasoning” may be unstable, confabulated, or disconnected from the actual basis of the output. In other words, chain-of-thought can create an **illusion of transparency** rather than a record of real reasons. (See [Ping](https://www.yalejreg.com/nc/iterative-reasoning-arbitrary-results-chain-of-thought-prompt-engineering-for-apa-compliance-by-elliot-e-c-ping/))

This is where the symposium starts to sound like a dialogue about what “reasons” even are under the APA. The legal system cares about reasons because they are supposed to be (a) grounded in the record, (b) stable enough to be reviewed, and (c) attributable to the agency at the time of decision (not invented later as litigation strategy). If AI systems can generate plausible justifications on demand, administrative lawyers have to ask whether the words on the page still function as evidence of the agency’s decision-making process.

### 3.3. Method, record, and disclosure: what should an agency have to explain about AI?

One of the most concrete doctrinal questions in the symposium is methodological: if an agency uses AI in a way that matters, **what must it explain**?

Ascher and Lewis draw an analogy to longstanding debates in the U.S. over agencies’ use of quantitative models. In those contexts, agencies have to be able to defend assumptions and methods when challenged. The parallel claim is straightforward: if AI tools are shaping what the agency does (not merely formatting prose), then agencies should be able to explain model choice, prompting, validation, and limits. (See [Ascher & Lewis](https://www.yalejreg.com/nc/toward-minimum-administrative-law-standards-for-agency-usage-of-ai-by-jordan-ascher-john-lewis/))

Jones and Ünel offer a pragmatic framing that both narrows and sharpens the issue. They suggest that courts are likely to continue focusing on whether the final rule is supported by evidence, responsive to significant comments, and coherent on its face—rather than routinely “auditing” the internal use of AI. But they also underline that opacity and sycophancy create real vulnerability: if there are already indications the agency’s reasoning is thin or inconsistent, AI reliance might make the decision look even less grounded in expert judgment. They also stress that not all AI uses are equal: an agency-controlled model trained on curated data raises different accountability questions than a general-purpose public model trained on uncontrolled sources. (See [Jones & Ünel](https://www.yalejreg.com/nc/do-large-language-models-dream-of-the-administrative-procedure-act-by-jack-jones-burcin-unel/))

Orbea and Froude’s contribution can be read as a way to operationalize this. They ask what “reasonableness” review might look like when AI is involved, proposing that scrutiny will plausibly vary with (i) the nature of the authority (how much value-laden discretion is involved), (ii) the role the AI plays (advisory vs. decisive), and (iii) the stakes and consequences. This framing reinforces a central “organizing” point: AI governance is not one-size-fits-all; it is likely to be **use-case-specific** and **risk-tiered**. (See [Orbea & Froude](https://www.yalejreg.com/nc/determining-the-reasonableness-of-regulating-with-ai-by-gilbert-orbea-emily-froude/))

Finally, the comparative piece by Tomlinson and McGurk underscores that “disclosure” is not merely an American obsession. The UK’s algorithmic transparency initiatives are a reminder that democratic systems are experimenting with registries and recording standards to make public-sector AI more legible—even if those initiatives remain partial and contested. (See [Tomlinson & McGurk](https://www.yalejreg.com/nc/artificial-intelligence-and-administrative-law-the-uks-search-for-a-new-framework-by-joe-tomlinson-brendan-mcgurk/))

### 3.4. Who should write the new rules: courts, agencies, or something in between?

Even if everyone agreed that AI requires new guardrails, there is a second-order fight about **institutional authorship**: who should define those guardrails?

Adam Crews argues against expecting courts to invent a new “AI administrative law” through judge-made procedural requirements. His warning is doctrinal as well as political: Vermont Yankee is a constraint on judicial procedural creativity, and the Supreme Court’s current posture makes ambitious court-driven innovation unstable. In Crews’s account, the more plausible path is for agencies to develop practices—policies, documentation routines, disclosure norms—that can then become “administrative common law” recognized by courts as baseline reasonableness. ACUS, in this story, is a key coordinating institution. (See [Crews](https://www.yalejreg.com/nc/agencies-not-courts-should-develop-administrative-common-law-for-ai-by-adam-crews/))

This theme ties back to the method/record debate: to the extent AI use becomes normal, courts may gradually treat certain kinds of documentation or disclosure as the expected baseline. The symposium’s implicit question is whether that baseline will emerge through litigation pressure, executive guidance, cross-agency coordination, or some combination.

### 3.5. Application-specific pressure points: where the APA hits AI first (and hardest)

If the earlier themes are about doctrine and institutions, the symposium also makes clear that the “AI and the APA” debate will not remain abstract. It will likely develop around **specific administrative settings**—because those settings surface distinct failure modes and distinct legal hooks.

**Notice-and-comment as “tick-the-box.”** Aida’s piece focuses on § 553 and the notice-and-comment process. Her worry is not simply that AI will be used to draft notices or responses, but that it can turn the written record into a hollow proxy: more detailed responses can coexist with less human engagement, less investigation, and less genuine openness to critique. Her further caution is that prompting can embed the conclusion (“reject the criticism”) in a way that systematically produces closed-mindedness. This is the symposium’s clearest statement of a “procedural legitimacy” worry: AI can make the process look more responsive while making it less deliberative. (See [Aida](https://www.yalejreg.com/nc/ticking-the-boxes-ai-and-the-notice-and-comment-process-by-tara-aida/))

**Procurement and the “documentation paradox.”** Tillipman’s procurement example shows how AI can generate extensive scoring and narratives that appear to satisfy documentation requirements while weakening the agency’s ability to explain tradeoffs under scrutiny. In procurement, the decision-maker’s signature matters; if AI reduces the signature to a formality, bid protests can become a test of whether administrative law will accept machine-shaped judgment as “reasoned.” (See [Tillipman](https://www.yalejreg.com/nc/abdicated-judgment-ai-tools-and-the-future-of-reasoned-decision-making-in-federal-procurement-by-jessica-tillipman/))

**Regulatory reform and “useful” automation.** Reeve T. Bull’s contribution is a useful counterweight to the most dystopian readings. Rather than treating AI as a substitute for policymaking, Bull imagines AI as a tool for organizing the regulatory code—classifying obligations, identifying where authority is thin, surfacing incorporation-by-reference problems, and enabling more systematic retrospective review. In this frame, AI helps agencies see what they already have and prioritize reform, without pretending that reform is a mechanical deletion task. (See [Bull](https://www.yalejreg.com/nc/ai-empowered-regulatory-reform-spreading-the-virginia-model-by-reeve-t-bull/))

Taken together, these “pressure points” support a broader organizing claim: many of the coming legal disputes will be less about whether AI is used at all and more about **what role AI plays** in the specific administrative workflow—screening, summarizing, recommending, drafting, or deciding—and how that role interacts with the APA’s demands.

## Conclusion

Read as a single debate, the symposium suggests that U.S. administrative law is entering a new phase—one where the central questions are no longer only about agency authority or deference, but about the *production of reasons* and the *institutional conditions* under which reasons can be trusted.

The emerging consensus is modest but meaningful: AI does not nullify the APA. Agencies will still be judged by whether they can connect their choices to evidence, respond to significant inputs, and take responsibility for their decisions. But the symposium makes equally clear why the debate is heating up: generative AI can cheaply generate the artifacts that administrative law has long treated as evidence of deliberation. If that link between writing and judgment weakens, the legal system will have to decide how to re-secure accountability—through disclosure, validation, record-building, institutional guardrails, or all of the above.

What makes the symposium especially useful (and what makes the next few years especially uncertain) is that it highlights competing intuitions that are both plausible:

- On one hand, the APA can be seen as technology-neutral: courts can just keep applying familiar tests to the final output.
- On the other hand, the shift in how administrative texts can be produced may require new expectations about method, transparency, and human involvement—because the old proxies for accountability may no longer work.

Organizing the debate around these themes does not answer the hard questions. But it does clarify what many American administrativists are already treating as the real agenda: deciding when AI is merely a tool inside the administrative process—and when it becomes a structural change that forces administrative law to rethink what it means to govern through reasons.

## Symposium posts so far (for further reading)

- Bridget C.E. Dooling & Jordan Ascher, “[Introduction to the Symposium on Artificial Intelligence and the Administrative Procedure Act](https://www.yalejreg.com/nc/introduction-to-the-symposium-on-artificial-intelligence-and-the-administrative-procedure-act-by-bridget-c-e-dooling-jordan-ascher/)” (Feb. 5, 2026)
- Cary Coglianese, “[AI, Taxi Drivers, and Administrative Law](https://www.yalejreg.com/nc/ai-taxi-drivers-and-administrative-law-by-cary-coglianese/)” (Feb. 6, 2026)
- Jordan Ascher & John Lewis, “[Toward Minimum Administrative Law Standards for Agency Usage of AI](https://www.yalejreg.com/nc/toward-minimum-administrative-law-standards-for-agency-usage-of-ai-by-jordan-ascher-john-lewis/)” (Feb. 6, 2026)
- Jack Jones & Burçin Ünel, “[Do Large Language Models Dream of the Administrative Procedure Act?](https://www.yalejreg.com/nc/do-large-language-models-dream-of-the-administrative-procedure-act-by-jack-jones-burcin-unel/)” (Feb. 9, 2026)
- Gilbert Orbea & Emily Froude, “[Determining the Reasonableness of Regulating with AI](https://www.yalejreg.com/nc/determining-the-reasonableness-of-regulating-with-ai-by-gilbert-orbea-emily-froude/)” (Feb. 10, 2026)
- Elliot E.C. Ping, “[Iterative Reasoning, Arbitrary Results: Chain-of-Thought Prompt Engineering for APA Compliance](https://www.yalejreg.com/nc/iterative-reasoning-arbitrary-results-chain-of-thought-prompt-engineering-for-apa-compliance-by-elliot-e-c-ping/)” (Feb. 11, 2026)
- Adam Crews, “[Agencies, Not Courts, Should Develop Administrative Common Law for AI](https://www.yalejreg.com/nc/agencies-not-courts-should-develop-administrative-common-law-for-ai-by-adam-crews/)” (Feb. 11, 2026)
- Reeve T. Bull, “[AI-empowered Regulatory Reform: Spreading the Virginia Model](https://www.yalejreg.com/nc/ai-empowered-regulatory-reform-spreading-the-virginia-model-by-reeve-t-bull/)” (Feb. 12, 2026)
- Jessica Tillipman, “[Abdicated Judgment: AI Tools and the Future of Reasoned Decision-Making in Federal Procurement](https://www.yalejreg.com/nc/abdicated-judgment-ai-tools-and-the-future-of-reasoned-decision-making-in-federal-procurement-by-jessica-tillipman/)” (Feb. 12, 2026)
- Tara Aida, “[Ticking the Boxes: AI and the Notice-and-Comment Process](https://www.yalejreg.com/nc/ticking-the-boxes-ai-and-the-notice-and-comment-process-by-tara-aida/)” (Feb. 13, 2026)
- Joe Tomlinson & Brendan McGurk, “[Artificial Intelligence and Administrative Law: The UK’s Search for a New Framework](https://www.yalejreg.com/nc/artificial-intelligence-and-administrative-law-the-uks-search-for-a-new-framework-by-joe-tomlinson-brendan-mcgurk/)” (Feb. 16, 2026)
- Bridget C.E. Dooling, “[Use Cases, Humans in the Loop, and Other Sleights of Hand](https://www.yalejreg.com/nc/use-cases-humans-in-the-loop-and-other-sleights-of-hand-by-bridget-c-e-dooling/)” (Feb. 16, 2026)
